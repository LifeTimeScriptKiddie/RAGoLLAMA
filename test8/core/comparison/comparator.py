"""
Embedding Comparator

Compares embeddings generated by different methods (Docling vs Microsoft RAG)
and provides analysis of their effectiveness for document retrieval.
"""

from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import logging
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)


class EmbeddingComparator:
    """Compares embeddings from different methods and analyzes their performance."""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize embedding comparator.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.similarity_threshold = config.get("similarity_threshold", 0.7)
        self.comparison_metrics = config.get("comparison_metrics", ["cosine", "euclidean", "manhattan"])
        
    def compare_embeddings(self, docling_embeddings: List[np.ndarray], 
                          microsoft_embeddings: List[np.ndarray],
                          text_chunks: List[str]) -> Dict[str, Any]:
        """
        Compare embeddings from both methods.
        
        Args:
            docling_embeddings: Embeddings from Docling method
            microsoft_embeddings: Embeddings from Microsoft method
            text_chunks: Original text chunks
            
        Returns:
            Comparison results dictionary
        """
        if len(docling_embeddings) != len(microsoft_embeddings):
            raise ValueError("Embedding lists must have the same length")
        
        if len(docling_embeddings) != len(text_chunks):
            raise ValueError("Embeddings and text chunks must have the same length")
        
        logger.info(f"Comparing {len(docling_embeddings)} embedding pairs")
        
        results = {
            "comparison_id": self._generate_comparison_id(),
            "timestamp": datetime.utcnow().isoformat(),
            "num_chunks": len(text_chunks),
            "docling_stats": self._calculate_embedding_stats(docling_embeddings, "docling"),
            "microsoft_stats": self._calculate_embedding_stats(microsoft_embeddings, "microsoft"),
            "similarity_analysis": self._analyze_similarities(docling_embeddings, microsoft_embeddings),
            "quality_metrics": self._calculate_quality_metrics(docling_embeddings, microsoft_embeddings, text_chunks),
            "recommendations": {}
        }
        
        # Determine best method based on metrics
        results["best_method"] = self._determine_best_method(results)
        results["recommendations"] = self._generate_recommendations(results)
        
        logger.info(f"Comparison completed. Best method: {results['best_method']}")
        return results
    
    def _generate_comparison_id(self) -> str:
        """Generate unique comparison ID."""
        import uuid
        return str(uuid.uuid4())
    
    def _calculate_embedding_stats(self, embeddings: List[np.ndarray], method_name: str) -> Dict[str, Any]:
        """Calculate statistics for a set of embeddings."""
        if not embeddings:
            return {}
        
        # Convert to numpy array for easier calculation
        embedding_matrix = np.array(embeddings)
        
        stats = {
            "method": method_name,
            "count": len(embeddings),
            "dimension": embedding_matrix.shape[1] if len(embedding_matrix.shape) > 1 else 0,
            "mean_magnitude": float(np.mean(np.linalg.norm(embedding_matrix, axis=1))),
            "std_magnitude": float(np.std(np.linalg.norm(embedding_matrix, axis=1))),
            "mean_values": embedding_matrix.mean(axis=0).tolist() if len(embedding_matrix.shape) > 1 else [],
            "std_values": embedding_matrix.std(axis=0).tolist() if len(embedding_matrix.shape) > 1 else [],
            "sparsity": self._calculate_sparsity(embedding_matrix)
        }
        
        return stats
    
    def _calculate_sparsity(self, embedding_matrix: np.ndarray) -> float:
        """Calculate sparsity of embeddings (percentage of near-zero values)."""
        if embedding_matrix.size == 0:
            return 0.0
        
        threshold = 1e-6
        near_zero_count = np.sum(np.abs(embedding_matrix) < threshold)
        total_elements = embedding_matrix.size
        
        return float(near_zero_count / total_elements)
    
    def _analyze_similarities(self, docling_embeddings: List[np.ndarray], 
                            microsoft_embeddings: List[np.ndarray]) -> Dict[str, Any]:
        """Analyze similarities between corresponding embeddings."""
        similarities = []
        
        for doc_emb, ms_emb in zip(docling_embeddings, microsoft_embeddings):
            # Reshape for cosine similarity calculation
            doc_emb_reshaped = doc_emb.reshape(1, -1)
            ms_emb_reshaped = ms_emb.reshape(1, -1)
            
            # Calculate cosine similarity
            similarity = cosine_similarity(doc_emb_reshaped, ms_emb_reshaped)[0][0]
            similarities.append(float(similarity))
        
        similarities = np.array(similarities)
        
        return {
            "pairwise_similarities": similarities.tolist(),
            "mean_similarity": float(np.mean(similarities)),
            "std_similarity": float(np.std(similarities)),
            "min_similarity": float(np.min(similarities)),
            "max_similarity": float(np.max(similarities)),
            "high_similarity_count": int(np.sum(similarities > self.similarity_threshold)),
            "similarity_threshold": self.similarity_threshold
        }
    
    def _calculate_quality_metrics(self, docling_embeddings: List[np.ndarray],
                                 microsoft_embeddings: List[np.ndarray], 
                                 text_chunks: List[str]) -> Dict[str, Any]:
        """Calculate quality metrics for both embedding methods."""
        metrics = {}
        
        # Internal consistency (how similar are embeddings within each method)
        metrics["docling_consistency"] = self._calculate_internal_consistency(docling_embeddings)
        metrics["microsoft_consistency"] = self._calculate_internal_consistency(microsoft_embeddings)
        
        # Discriminative power (how well embeddings distinguish between different chunks)
        metrics["docling_discriminative_power"] = self._calculate_discriminative_power(docling_embeddings)
        metrics["microsoft_discriminative_power"] = self._calculate_discriminative_power(microsoft_embeddings)
        
        # Semantic coherence (correlation with text similarity)
        metrics["docling_semantic_coherence"] = self._calculate_semantic_coherence(docling_embeddings, text_chunks)
        metrics["microsoft_semantic_coherence"] = self._calculate_semantic_coherence(microsoft_embeddings, text_chunks)
        
        return metrics
    
    def _calculate_internal_consistency(self, embeddings: List[np.ndarray]) -> float:
        """Calculate internal consistency of embeddings."""
        if len(embeddings) < 2:
            return 1.0
        
        # Calculate pairwise similarities within the method
        embedding_matrix = np.array(embeddings)
        similarity_matrix = cosine_similarity(embedding_matrix)
        
        # Get upper triangle (excluding diagonal)
        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
        
        # Return mean similarity (higher = more consistent)
        return float(np.mean(upper_triangle))
    
    def _calculate_discriminative_power(self, embeddings: List[np.ndarray]) -> float:
        """Calculate discriminative power of embeddings."""
        if len(embeddings) < 2:
            return 1.0
        
        # Calculate pairwise similarities
        embedding_matrix = np.array(embeddings)
        similarity_matrix = cosine_similarity(embedding_matrix)
        
        # Get upper triangle (excluding diagonal)
        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
        
        # Discriminative power is inversely related to similarity
        # Higher variance in similarities = better discriminative power
        return float(np.std(upper_triangle))
    
    def _calculate_semantic_coherence(self, embeddings: List[np.ndarray], text_chunks: List[str]) -> float:
        """Calculate semantic coherence between embeddings and text similarity."""
        if len(embeddings) < 2 or len(text_chunks) < 2:
            return 1.0
        
        try:
            # Calculate embedding similarities
            embedding_matrix = np.array(embeddings)
            embedding_similarities = cosine_similarity(embedding_matrix)
            
            # Calculate text similarities (simple approach using character overlap)
            text_similarities = self._calculate_text_similarities(text_chunks)
            
            # Calculate correlation between embedding and text similarities
            embedding_flat = embedding_similarities[np.triu_indices_from(embedding_similarities, k=1)]
            text_flat = text_similarities[np.triu_indices_from(text_similarities, k=1)]
            
            # Use Pearson correlation coefficient
            correlation = np.corrcoef(embedding_flat, text_flat)[0, 1]
            
            # Handle NaN values
            return float(correlation) if not np.isnan(correlation) else 0.0
            
        except Exception as e:
            logger.warning(f"Error calculating semantic coherence: {e}")
            return 0.0
    
    def _calculate_text_similarities(self, text_chunks: List[str]) -> np.ndarray:
        """Calculate text similarities using simple character-based approach."""
        n = len(text_chunks)
        similarities = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i == j:
                    similarities[i, j] = 1.0
                else:
                    # Simple Jaccard similarity on character n-grams
                    similarities[i, j] = self._jaccard_similarity(text_chunks[i], text_chunks[j])
        
        return similarities
    
    def _jaccard_similarity(self, text1: str, text2: str, n: int = 3) -> float:
        """Calculate Jaccard similarity between two texts using character n-grams."""
        if not text1 or not text2:
            return 0.0
        
        # Generate character n-grams
        ngrams1 = set(text1[i:i+n] for i in range(len(text1) - n + 1))
        ngrams2 = set(text2[i:i+n] for i in range(len(text2) - n + 1))
        
        if not ngrams1 and not ngrams2:
            return 1.0
        
        intersection = len(ngrams1.intersection(ngrams2))
        union = len(ngrams1.union(ngrams2))
        
        return intersection / union if union > 0 else 0.0
    
    def _determine_best_method(self, results: Dict[str, Any]) -> str:
        """Determine the best embedding method based on comparison results."""
        docling_score = 0
        microsoft_score = 0
        
        # Compare discriminative power
        docling_discrim = results["quality_metrics"]["docling_discriminative_power"]
        microsoft_discrim = results["quality_metrics"]["microsoft_discriminative_power"]
        
        if docling_discrim > microsoft_discrim:
            docling_score += 1
        else:
            microsoft_score += 1
        
        # Compare semantic coherence
        docling_coherence = results["quality_metrics"]["docling_semantic_coherence"]
        microsoft_coherence = results["quality_metrics"]["microsoft_semantic_coherence"]
        
        if docling_coherence > microsoft_coherence:
            docling_score += 1
        else:
            microsoft_score += 1
        
        # Compare consistency (balanced - not too high, not too low)
        docling_consistency = results["quality_metrics"]["docling_consistency"]
        microsoft_consistency = results["quality_metrics"]["microsoft_consistency"]
        
        # Ideal consistency is around 0.3-0.7 (similar but not identical)
        ideal_consistency = 0.5
        docling_consistency_score = 1 - abs(docling_consistency - ideal_consistency)
        microsoft_consistency_score = 1 - abs(microsoft_consistency - ideal_consistency)
        
        if docling_consistency_score > microsoft_consistency_score:
            docling_score += 1
        else:
            microsoft_score += 1
        
        return "docling" if docling_score > microsoft_score else "microsoft"
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate recommendations based on comparison results."""
        recommendations = {}
        
        best_method = results["best_method"]
        similarity_analysis = results["similarity_analysis"]
        
        # General recommendation
        recommendations["primary_method"] = best_method
        recommendations["use_case"] = "general_purpose"
        
        # Specific recommendations based on metrics
        if similarity_analysis["mean_similarity"] > 0.8:
            recommendations["note"] = "Both methods produce very similar embeddings. Consider computational efficiency."
        elif similarity_analysis["mean_similarity"] < 0.3:
            recommendations["note"] = "Methods produce significantly different embeddings. Consider ensemble approach."
        else:
            recommendations["note"] = "Methods show moderate differences. Chosen method shows better quality metrics."
        
        # Performance recommendations
        recommendations["performance"] = {
            "primary": best_method,
            "fallback": "microsoft" if best_method == "docling" else "docling",
            "ensemble_recommended": similarity_analysis["mean_similarity"] < 0.6
        }
        
        return recommendations
    
    def get_comparison_summary(self, comparison_results: Dict[str, Any]) -> str:
        """Generate a human-readable summary of comparison results."""
        best_method = comparison_results["best_method"]
        num_chunks = comparison_results["num_chunks"]
        mean_similarity = comparison_results["similarity_analysis"]["mean_similarity"]
        
        summary = f"""
Embedding Comparison Summary
===========================
Analyzed {num_chunks} text chunks using both Docling and Microsoft RAG methods.

Best Method: {best_method.title()}
Average Similarity: {mean_similarity:.3f}

Key Metrics:
- Docling Discriminative Power: {comparison_results['quality_metrics']['docling_discriminative_power']:.3f}
- Microsoft Discriminative Power: {comparison_results['quality_metrics']['microsoft_discriminative_power']:.3f}
- Docling Semantic Coherence: {comparison_results['quality_metrics']['docling_semantic_coherence']:.3f}
- Microsoft Semantic Coherence: {comparison_results['quality_metrics']['microsoft_semantic_coherence']:.3f}

Recommendation: {comparison_results['recommendations']['note']}
        """.strip()
        
        return summary