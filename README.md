

### ğŸ“Š **Mermaid Diagram for Your RAG Pipeline**

```mermaid
flowchart TD
    A[ğŸ“„ PDF Input] --> B[ğŸ“ Text Extraction]
    B --> C[ğŸ”ª Chunking]
    C --> D[ğŸ”¢ Embedding]
    D --> E[ğŸ§  Vector DB]
    E --> F[ğŸ” Similarity Search]
    F --> G[ğŸ§¾ Prompt Construction]
    G --> H[ğŸ¤– LLM Response]

    style A fill:#fef9c3,stroke:#000
    style B fill:#fce7f3,stroke:#000
    style C fill:#e0f2fe,stroke:#000
    style D fill:#ede9fe,stroke:#000
    style E fill:#d1fae5,stroke:#000
    style F fill:#fcd34d,stroke:#000
    style G fill:#f3e8ff,stroke:#000
    style H fill:#dbeafe,stroke:#000

```

---

### ğŸ§  How It Works â€“ Step by Step

| Step                       | Description                                                                                            |
| -------------------------- | ------------------------------------------------------------------------------------------------------ |
| **ğŸ“„ PDF Input**           | The user uploads a `.pdf` file.                                                                        |
| **ğŸ“ Text Extraction**     | Text is extracted from the PDF using `PyPDF2`, or `ocrmypdf` if it's image-based.                      |
| **ğŸ”ª Chunking**            | The text is broken into overlapping segments using LangChain's `RecursiveCharacterTextSplitter`.       |
| **ğŸ”¢ Embedding**           | Each chunk is embedded into a vector using SentenceTransformers.                                       |
| **ğŸ§  Vector DB**           | All embeddings are stored temporarily (in memory or file-based).                                       |
| **ğŸ” Similarity Search**   | When a question is asked, its embedding is compared against the vector DB to retrieve relevant chunks. |
| **ğŸ§¾ Prompt Construction** | The top-k chunks are inserted into a prompt with the user's question.                                  |
| **ğŸ¤– LLM Response**        | The prompt is sent to Ollamaâ€™s local LLM for generation.                                               |

---
